{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://github.com/BIGBALLON/cifar-10-cnn/blob/master/2_Network_in_Network/Network_in_Network_keras.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.initializers import RandomNormal  \n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size    = 128\n",
    "epochs        = 200\n",
    "# number of steps per epoch\n",
    "iterations    = 391\n",
    "num_classes   = 10\n",
    "dropout       = 0.5\n",
    "# for L2 weight regularizer\n",
    "weight_decay  = 0.0001\n",
    "log_filepath  = './nin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [ x_train[:,:,:,i].mean() for i in range(3) ]\n",
    "    std  = [ x_train[:,:,:,i].std() for i in range(3) ]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch <= 80:\n",
    "        return 0.01\n",
    "    if epoch <= 140:\n",
    "        return 0.005\n",
    "    return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = Sequential()\n",
    "\n",
    "  def conv(nfilters,size,**kwargs):\n",
    "        return Conv2D(nfilters, (size,size), padding='same', \n",
    "                      kernel_regularizer=keras.regularizers.l2(weight_decay), \n",
    "                      kernel_initializer=\"he_normal\",\n",
    "                      activation='relu',\n",
    "                      **kwargs)\n",
    "  \n",
    "  def maxpool():\n",
    "        return MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding='same')\n",
    "\n",
    "  model.add(conv(192, 5, input_shape=x_train.shape[1:]))\n",
    "  model.add(conv(160, 1))\n",
    "  model.add(conv(96, 1))\n",
    "  model.add(maxpool())\n",
    "  \n",
    "  model.add(Dropout(dropout))\n",
    "  \n",
    "  model.add(conv(192,5))\n",
    "  model.add(conv(192,1))\n",
    "  model.add(conv(192,1))\n",
    "  model.add(maxpool())\n",
    "  \n",
    "  model.add(Dropout(dropout))\n",
    "  \n",
    "  model.add(conv(192,3))\n",
    "  model.add(conv(192,1))\n",
    "  model.add(conv(10,1))\n",
    "\n",
    "  model.add(GlobalAveragePooling2D())\n",
    "  model.add(Activation('softmax'))\n",
    "  \n",
    "  sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = color_preprocessing(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [ x_train[:,:,:,i].mean() for i in range(3) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 32, 32, 192)       14592     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 32, 160)       30880     \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 96)        15456     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 16, 16, 192)       460992    \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 16, 16, 192)       37056     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 16, 16, 192)       37056     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 8, 8, 192)         331968    \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 8, 8, 192)         37056     \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 8, 8, 10)          1930      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 966,986\n",
      "Trainable params: 966,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build network\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set callbacks\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "cbks = [change_lr,tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant',\n",
    "                             cval=0.)\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "391/391 [==============================] - 517s 1s/step - loss: 2.4847 - acc: 0.1349 - val_loss: 2.3123 - val_acc: 0.2409\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 516s 1s/step - loss: 2.1733 - acc: 0.2993 - val_loss: 2.1160 - val_acc: 0.3392\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 516s 1s/step - loss: 1.9519 - acc: 0.3915 - val_loss: 1.9107 - val_acc: 0.4044\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 512s 1s/step - loss: 1.7789 - acc: 0.4640 - val_loss: 1.7660 - val_acc: 0.4780\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 512s 1s/step - loss: 1.6695 - acc: 0.5058 - val_loss: 1.5849 - val_acc: 0.5463\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 517s 1s/step - loss: 1.4936 - acc: 0.5511 - val_loss: 1.5363 - val_acc: 0.5544\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 518s 1s/step - loss: 1.3596 - acc: 0.5887 - val_loss: 1.3058 - val_acc: 0.6200\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 506s 1s/step - loss: 1.2808 - acc: 0.6158 - val_loss: 1.3338 - val_acc: 0.6206\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 501s 1s/step - loss: 1.2185 - acc: 0.6423 - val_loss: 1.2967 - val_acc: 0.6255\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 1.1699 - acc: 0.6594 - val_loss: 1.1386 - val_acc: 0.6775\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 1.1170 - acc: 0.6810 - val_loss: 1.1175 - val_acc: 0.6846\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 1.0747 - acc: 0.6960 - val_loss: 1.1348 - val_acc: 0.6883\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 1.0414 - acc: 0.7065 - val_loss: 1.0606 - val_acc: 0.7102\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 500s 1s/step - loss: 1.0108 - acc: 0.7173 - val_loss: 1.0418 - val_acc: 0.7241\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.9801 - acc: 0.7291 - val_loss: 0.9719 - val_acc: 0.7392\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.9575 - acc: 0.7379 - val_loss: 0.9239 - val_acc: 0.7592\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 0.9305 - acc: 0.7482 - val_loss: 0.9088 - val_acc: 0.7600\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 0.9149 - acc: 0.7530 - val_loss: 0.8889 - val_acc: 0.7664\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.8931 - acc: 0.7593 - val_loss: 0.9928 - val_acc: 0.7504\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.8752 - acc: 0.7646 - val_loss: 0.9044 - val_acc: 0.7672\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 0.8629 - acc: 0.7712 - val_loss: 0.8956 - val_acc: 0.7716\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.8492 - acc: 0.7775 - val_loss: 0.8235 - val_acc: 0.7897\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.8357 - acc: 0.7811 - val_loss: 0.8142 - val_acc: 0.7921\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 529s 1s/step - loss: 0.8182 - acc: 0.7882 - val_loss: 0.8465 - val_acc: 0.7852\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 511s 1s/step - loss: 0.8094 - acc: 0.7889 - val_loss: 0.8381 - val_acc: 0.7901\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 506s 1s/step - loss: 0.7956 - acc: 0.7941 - val_loss: 0.7748 - val_acc: 0.8094\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.7848 - acc: 0.7975 - val_loss: 0.7853 - val_acc: 0.8038\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.7724 - acc: 0.8000 - val_loss: 0.7696 - val_acc: 0.8098\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.7634 - acc: 0.8069 - val_loss: 0.8119 - val_acc: 0.7991\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.7511 - acc: 0.8103 - val_loss: 0.8076 - val_acc: 0.8082\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.7422 - acc: 0.8129 - val_loss: 0.7615 - val_acc: 0.8115\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.7358 - acc: 0.8138 - val_loss: 0.7581 - val_acc: 0.8134\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.7262 - acc: 0.8175 - val_loss: 0.7485 - val_acc: 0.8171\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.7209 - acc: 0.8202 - val_loss: 0.7707 - val_acc: 0.8087\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.7095 - acc: 0.8236 - val_loss: 0.7577 - val_acc: 0.8173\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.7018 - acc: 0.8263 - val_loss: 0.7415 - val_acc: 0.8204\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.6961 - acc: 0.8290 - val_loss: 0.7059 - val_acc: 0.8323\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.6886 - acc: 0.8302 - val_loss: 0.7403 - val_acc: 0.8214\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.6818 - acc: 0.8327 - val_loss: 0.6998 - val_acc: 0.8335\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.6760 - acc: 0.8332 - val_loss: 0.7382 - val_acc: 0.8296\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 502s 1s/step - loss: 0.6718 - acc: 0.8350 - val_loss: 0.8080 - val_acc: 0.8106\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.6645 - acc: 0.8363 - val_loss: 0.7123 - val_acc: 0.8336\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 573s 1s/step - loss: 0.6540 - acc: 0.8397 - val_loss: 0.7132 - val_acc: 0.8348\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 661s 2s/step - loss: 0.6523 - acc: 0.8430 - val_loss: 0.6947 - val_acc: 0.8349\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 566s 1s/step - loss: 0.6492 - acc: 0.8456 - val_loss: 0.6709 - val_acc: 0.8459\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.6367 - acc: 0.8493 - val_loss: 0.6606 - val_acc: 0.8460\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.6335 - acc: 0.8486 - val_loss: 0.6826 - val_acc: 0.8434\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.6271 - acc: 0.8502 - val_loss: 0.7195 - val_acc: 0.8313\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.6248 - acc: 0.8517 - val_loss: 0.8016 - val_acc: 0.8109\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.6219 - acc: 0.8522 - val_loss: 0.7134 - val_acc: 0.8341\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.6140 - acc: 0.8532 - val_loss: 0.6342 - val_acc: 0.8523\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.6143 - acc: 0.8537 - val_loss: 0.6698 - val_acc: 0.8472\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.6083 - acc: 0.8583 - val_loss: 0.6784 - val_acc: 0.8423\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.6070 - acc: 0.8559 - val_loss: 0.6679 - val_acc: 0.8474\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.5972 - acc: 0.8600 - val_loss: 0.6840 - val_acc: 0.8430\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.5948 - acc: 0.8600 - val_loss: 0.6553 - val_acc: 0.8514\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.5929 - acc: 0.8605 - val_loss: 0.7267 - val_acc: 0.8404\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.5861 - acc: 0.8636 - val_loss: 0.6991 - val_acc: 0.8420\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.5831 - acc: 0.8639 - val_loss: 0.6667 - val_acc: 0.8483\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.5780 - acc: 0.8672 - val_loss: 0.7025 - val_acc: 0.8393\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.5761 - acc: 0.8664 - val_loss: 0.7077 - val_acc: 0.8394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.5729 - acc: 0.8692 - val_loss: 0.6481 - val_acc: 0.8565\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 546s 1s/step - loss: 0.5679 - acc: 0.8687 - val_loss: 0.6631 - val_acc: 0.8477\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 545s 1s/step - loss: 0.5669 - acc: 0.8695 - val_loss: 0.7046 - val_acc: 0.8371\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.5601 - acc: 0.8724 - val_loss: 0.6658 - val_acc: 0.8531\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 520s 1s/step - loss: 0.5559 - acc: 0.8740 - val_loss: 0.6359 - val_acc: 0.8624\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 516s 1s/step - loss: 0.5622 - acc: 0.8720 - val_loss: 0.6782 - val_acc: 0.8468\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 519s 1s/step - loss: 0.5516 - acc: 0.8740 - val_loss: 0.6827 - val_acc: 0.8452\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 506s 1s/step - loss: 0.5522 - acc: 0.8742 - val_loss: 0.6569 - val_acc: 0.8554\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 510s 1s/step - loss: 0.5480 - acc: 0.8762 - val_loss: 0.6617 - val_acc: 0.8476\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 505s 1s/step - loss: 0.5462 - acc: 0.8770 - val_loss: 0.6417 - val_acc: 0.8595\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 579s 1s/step - loss: 0.5423 - acc: 0.8779 - val_loss: 0.6682 - val_acc: 0.8475\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 528s 1s/step - loss: 0.5399 - acc: 0.8783 - val_loss: 0.6655 - val_acc: 0.8564\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 486s 1s/step - loss: 0.5346 - acc: 0.8805 - val_loss: 0.6868 - val_acc: 0.8494\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 485s 1s/step - loss: 0.5381 - acc: 0.8800 - val_loss: 0.6517 - val_acc: 0.8564\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 497s 1s/step - loss: 0.5281 - acc: 0.8828 - val_loss: 0.6996 - val_acc: 0.8415\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.5245 - acc: 0.8833 - val_loss: 0.6614 - val_acc: 0.8573\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 0.5254 - acc: 0.8841 - val_loss: 0.6741 - val_acc: 0.8466\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.5243 - acc: 0.8845 - val_loss: 0.6543 - val_acc: 0.8537\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 537s 1s/step - loss: 0.5207 - acc: 0.8851 - val_loss: 0.6702 - val_acc: 0.8553\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 538s 1s/step - loss: 0.5206 - acc: 0.8872 - val_loss: 0.6482 - val_acc: 0.8598\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 538s 1s/step - loss: 0.4812 - acc: 0.8992 - val_loss: 0.6289 - val_acc: 0.8655\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 547s 1s/step - loss: 0.4755 - acc: 0.9003 - val_loss: 0.6268 - val_acc: 0.8655\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 558s 1s/step - loss: 0.4717 - acc: 0.9029 - val_loss: 0.6153 - val_acc: 0.8689\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 549s 1s/step - loss: 0.4685 - acc: 0.9040 - val_loss: 0.6011 - val_acc: 0.8751\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 486s 1s/step - loss: 0.4685 - acc: 0.9032 - val_loss: 0.6023 - val_acc: 0.8712\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 489s 1s/step - loss: 0.4625 - acc: 0.9050 - val_loss: 0.7028 - val_acc: 0.8526\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 489s 1s/step - loss: 0.4655 - acc: 0.9040 - val_loss: 0.6723 - val_acc: 0.8621\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 489s 1s/step - loss: 0.4598 - acc: 0.9059 - val_loss: 0.6335 - val_acc: 0.8656\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 491s 1s/step - loss: 0.4619 - acc: 0.9040 - val_loss: 0.6410 - val_acc: 0.8657\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 491s 1s/step - loss: 0.4552 - acc: 0.9076 - val_loss: 0.6367 - val_acc: 0.8657\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 492s 1s/step - loss: 0.4556 - acc: 0.9059 - val_loss: 0.6614 - val_acc: 0.8596\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 492s 1s/step - loss: 0.4546 - acc: 0.9069 - val_loss: 0.6243 - val_acc: 0.8679\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4496 - acc: 0.9093 - val_loss: 0.6268 - val_acc: 0.8707\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4521 - acc: 0.9079 - val_loss: 0.6171 - val_acc: 0.8657\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4474 - acc: 0.9097 - val_loss: 0.6420 - val_acc: 0.8631\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4469 - acc: 0.9109 - val_loss: 0.6171 - val_acc: 0.8746\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4454 - acc: 0.9110 - val_loss: 0.6004 - val_acc: 0.8773\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4456 - acc: 0.9110 - val_loss: 0.5960 - val_acc: 0.8789\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4446 - acc: 0.9095 - val_loss: 0.6029 - val_acc: 0.8747\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 492s 1s/step - loss: 0.4431 - acc: 0.9102 - val_loss: 0.6503 - val_acc: 0.8643\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 492s 1s/step - loss: 0.4405 - acc: 0.9118 - val_loss: 0.6498 - val_acc: 0.8681\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 492s 1s/step - loss: 0.4387 - acc: 0.9126 - val_loss: 0.6266 - val_acc: 0.8713\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4402 - acc: 0.9119 - val_loss: 0.6469 - val_acc: 0.8675\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4392 - acc: 0.9111 - val_loss: 0.6976 - val_acc: 0.8534\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4361 - acc: 0.9130 - val_loss: 0.6772 - val_acc: 0.8645\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4343 - acc: 0.9132 - val_loss: 0.6632 - val_acc: 0.8632\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4322 - acc: 0.9140 - val_loss: 0.6081 - val_acc: 0.8717\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4329 - acc: 0.9137 - val_loss: 0.6058 - val_acc: 0.8726\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 493s 1s/step - loss: 0.4310 - acc: 0.9156 - val_loss: 0.6429 - val_acc: 0.8694\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4303 - acc: 0.9136 - val_loss: 0.6466 - val_acc: 0.8644\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4295 - acc: 0.9146 - val_loss: 0.6223 - val_acc: 0.8718\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4269 - acc: 0.9150 - val_loss: 0.6063 - val_acc: 0.8754\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4268 - acc: 0.9147 - val_loss: 0.6323 - val_acc: 0.8662\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4232 - acc: 0.9170 - val_loss: 0.6028 - val_acc: 0.8766\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4292 - acc: 0.9148 - val_loss: 0.6171 - val_acc: 0.8711\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4258 - acc: 0.9161 - val_loss: 0.5964 - val_acc: 0.8767\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4187 - acc: 0.9176 - val_loss: 0.6128 - val_acc: 0.8716\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4217 - acc: 0.9179 - val_loss: 0.6215 - val_acc: 0.8692\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4188 - acc: 0.9180 - val_loss: 0.6185 - val_acc: 0.8743\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.4195 - acc: 0.9168 - val_loss: 0.6013 - val_acc: 0.8787\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 496s 1s/step - loss: 0.4169 - acc: 0.9167 - val_loss: 0.5909 - val_acc: 0.8810\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.4175 - acc: 0.9185 - val_loss: 0.6612 - val_acc: 0.8695\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.4141 - acc: 0.9194 - val_loss: 0.6090 - val_acc: 0.8728\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.4134 - acc: 0.9194 - val_loss: 0.6158 - val_acc: 0.8726\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.4117 - acc: 0.9198 - val_loss: 0.6206 - val_acc: 0.8732\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4115 - acc: 0.9201 - val_loss: 0.6013 - val_acc: 0.8762\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4137 - acc: 0.9206 - val_loss: 0.6721 - val_acc: 0.8686\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4070 - acc: 0.9212 - val_loss: 0.6465 - val_acc: 0.8674\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4112 - acc: 0.9208 - val_loss: 0.6271 - val_acc: 0.8736\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4073 - acc: 0.9205 - val_loss: 0.6563 - val_acc: 0.8667\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4074 - acc: 0.9204 - val_loss: 0.5995 - val_acc: 0.8792\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.4071 - acc: 0.9219 - val_loss: 0.6280 - val_acc: 0.8736\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4049 - acc: 0.9224 - val_loss: 0.5840 - val_acc: 0.8818\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4040 - acc: 0.9219 - val_loss: 0.6241 - val_acc: 0.8772\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4069 - acc: 0.9220 - val_loss: 0.6326 - val_acc: 0.8716\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4048 - acc: 0.9221 - val_loss: 0.6031 - val_acc: 0.8796\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.4039 - acc: 0.9228 - val_loss: 0.6542 - val_acc: 0.8674\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.3998 - acc: 0.9243 - val_loss: 0.6218 - val_acc: 0.8764\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3996 - acc: 0.9256 - val_loss: 0.5897 - val_acc: 0.8794\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.4026 - acc: 0.9232 - val_loss: 0.6401 - val_acc: 0.8705\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3710 - acc: 0.9335 - val_loss: 0.6068 - val_acc: 0.8835\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 610s 2s/step - loss: 0.3641 - acc: 0.9368 - val_loss: 0.6054 - val_acc: 0.8807\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 648s 2s/step - loss: 0.3611 - acc: 0.9381 - val_loss: 0.6099 - val_acc: 0.8804\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 646s 2s/step - loss: 0.3589 - acc: 0.9385 - val_loss: 0.6178 - val_acc: 0.8795\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 634s 2s/step - loss: 0.3559 - acc: 0.9396 - val_loss: 0.6005 - val_acc: 0.8817\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 509s 1s/step - loss: 0.3546 - acc: 0.9395 - val_loss: 0.6204 - val_acc: 0.8790\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 484s 1s/step - loss: 0.3534 - acc: 0.9398 - val_loss: 0.6357 - val_acc: 0.8798\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 484s 1s/step - loss: 0.3536 - acc: 0.9386 - val_loss: 0.6328 - val_acc: 0.8783\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 488s 1s/step - loss: 0.3521 - acc: 0.9414 - val_loss: 0.5996 - val_acc: 0.8849\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 489s 1s/step - loss: 0.3521 - acc: 0.9408 - val_loss: 0.6008 - val_acc: 0.8818\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 490s 1s/step - loss: 0.3527 - acc: 0.9393 - val_loss: 0.5953 - val_acc: 0.8850\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 490s 1s/step - loss: 0.3526 - acc: 0.9405 - val_loss: 0.5939 - val_acc: 0.8851\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 491s 1s/step - loss: 0.3513 - acc: 0.9398 - val_loss: 0.5917 - val_acc: 0.8847\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.3505 - acc: 0.9416 - val_loss: 0.6377 - val_acc: 0.8775\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.3481 - acc: 0.9417 - val_loss: 0.5934 - val_acc: 0.8844\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.3527 - acc: 0.9398 - val_loss: 0.6053 - val_acc: 0.8814\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.3478 - acc: 0.9423 - val_loss: 0.6141 - val_acc: 0.8831\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3472 - acc: 0.9424 - val_loss: 0.6148 - val_acc: 0.8811\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3496 - acc: 0.9416 - val_loss: 0.6056 - val_acc: 0.8807\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3473 - acc: 0.9422 - val_loss: 0.6056 - val_acc: 0.8829\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3471 - acc: 0.9431 - val_loss: 0.6167 - val_acc: 0.8838\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.3454 - acc: 0.9430 - val_loss: 0.5902 - val_acc: 0.8860\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3445 - acc: 0.9428 - val_loss: 0.5903 - val_acc: 0.8852\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3447 - acc: 0.9437 - val_loss: 0.5913 - val_acc: 0.8895\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3451 - acc: 0.9419 - val_loss: 0.6366 - val_acc: 0.8805\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 495s 1s/step - loss: 0.3465 - acc: 0.9422 - val_loss: 0.6194 - val_acc: 0.8834\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 507s 1s/step - loss: 0.3463 - acc: 0.9425 - val_loss: 0.6159 - val_acc: 0.8828\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 508s 1s/step - loss: 0.3431 - acc: 0.9429 - val_loss: 0.6129 - val_acc: 0.8811\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 509s 1s/step - loss: 0.3441 - acc: 0.9415 - val_loss: 0.6161 - val_acc: 0.8833\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 508s 1s/step - loss: 0.3412 - acc: 0.9436 - val_loss: 0.5969 - val_acc: 0.8849\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 517s 1s/step - loss: 0.3440 - acc: 0.9431 - val_loss: 0.6298 - val_acc: 0.8782\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 508s 1s/step - loss: 0.3461 - acc: 0.9424 - val_loss: 0.6164 - val_acc: 0.8805\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 510s 1s/step - loss: 0.3438 - acc: 0.9425 - val_loss: 0.6341 - val_acc: 0.8796\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 515s 1s/step - loss: 0.3439 - acc: 0.9425 - val_loss: 0.6289 - val_acc: 0.8782\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 518s 1s/step - loss: 0.3423 - acc: 0.9439 - val_loss: 0.5976 - val_acc: 0.8861\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 512s 1s/step - loss: 0.3396 - acc: 0.9450 - val_loss: 0.5931 - val_acc: 0.8841\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 514s 1s/step - loss: 0.3399 - acc: 0.9430 - val_loss: 0.6179 - val_acc: 0.8829\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 520s 1s/step - loss: 0.3435 - acc: 0.9432 - val_loss: 0.5928 - val_acc: 0.8874\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 512s 1s/step - loss: 0.3380 - acc: 0.9450 - val_loss: 0.6015 - val_acc: 0.8865\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 511s 1s/step - loss: 0.3405 - acc: 0.9435 - val_loss: 0.5931 - val_acc: 0.8845\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 516s 1s/step - loss: 0.3422 - acc: 0.9433 - val_loss: 0.6192 - val_acc: 0.8830\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 517s 1s/step - loss: 0.3410 - acc: 0.9433 - val_loss: 0.6075 - val_acc: 0.8831\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 520s 1s/step - loss: 0.3395 - acc: 0.9444 - val_loss: 0.6230 - val_acc: 0.8814\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 512s 1s/step - loss: 0.3404 - acc: 0.9445 - val_loss: 0.6267 - val_acc: 0.8834\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 535s 1s/step - loss: 0.3385 - acc: 0.9429 - val_loss: 0.6029 - val_acc: 0.8842\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 525s 1s/step - loss: 0.3387 - acc: 0.9440 - val_loss: 0.5899 - val_acc: 0.8876\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 507s 1s/step - loss: 0.3362 - acc: 0.9448 - val_loss: 0.6269 - val_acc: 0.8841\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 504s 1s/step - loss: 0.3379 - acc: 0.9439 - val_loss: 0.6237 - val_acc: 0.8802\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 527s 1s/step - loss: 0.3352 - acc: 0.9461 - val_loss: 0.6254 - val_acc: 0.8820\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 502s 1s/step - loss: 0.3346 - acc: 0.9454 - val_loss: 0.5993 - val_acc: 0.8860\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 0.3386 - acc: 0.9438 - val_loss: 0.6186 - val_acc: 0.8790\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 0.3357 - acc: 0.9462 - val_loss: 0.6052 - val_acc: 0.8845\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.3352 - acc: 0.9458 - val_loss: 0.6086 - val_acc: 0.8823\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 498s 1s/step - loss: 0.3357 - acc: 0.9456 - val_loss: 0.6145 - val_acc: 0.8824\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.3349 - acc: 0.9458 - val_loss: 0.6127 - val_acc: 0.8835\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.3338 - acc: 0.9468 - val_loss: 0.6045 - val_acc: 0.8837\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.3332 - acc: 0.9460 - val_loss: 0.6159 - val_acc: 0.8837\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.3310 - acc: 0.9473 - val_loss: 0.6047 - val_acc: 0.8847\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.3369 - acc: 0.9456 - val_loss: 0.6027 - val_acc: 0.8853\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                    steps_per_epoch=iterations,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=cbks,\n",
    "                    validation_data=(x_test, y_test))\n",
    "model.save('cifar10_nin.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
